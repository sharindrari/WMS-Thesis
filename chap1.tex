%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}
With the advancement of Big Data Analytics, numerous systems for cluster computing on big data have been developed \cite{zaharia2012resilient,battre2010nephele,thusoo2010hive,dean2008mapreduce,yu2008dryadlinq,olston2008pig} and data engineers are building more and more complex applications to manage and process large data sets on distributed resources. Such complex application scenarios require means in order to compose and execute complex workflows. Workflows automate procedures that users would otherwise need to carry out manually \cite{deelman2009workflows}. It refers to a sequence of steps or computations that a user would like to perform 
\footnote{\label{crobak}http://www.crobak.org/2012/07/workflow-engines-for-hadoop}. As an example, within a Hadoop \footnote{\label{hadoop}http://hadoop.apache.org/.} cluster, a user may need to export the production databases and load the data to the Hadoop File System (HDFS) as the first step. The second step would be to run a MapReduce job to clean up the data and the third step is a set of operations that run in parallel to count and filter the data. A workflow is intended to map all of the different operations together. Such a workflow is usually represented as a Directed Acyclic Graph (DAG) where the nodes can be tasks or control flow structures and edges represent the relationships between tasks, namely task or data dependency. A Workflow Management System (WMS) is a system that allows users and developers to create, define, run, and delete a workflow \cref{crobak}. 
	
As an introduction to what a workflow may look like, we will walk through two different use cases that are representative use cases in the Big Data environment. The first use case is Analytics/Data Warehousing. A workflow in this first use case consists of the following steps: (1) load the logs into the Fact tables, (2) load the database backups into the Dimension tables, (3) compute the aggregations and perform rollups/cubes inside Hadoop for instance, (4) load the data into a low latency store, and (4) in the end, perform the analytics using a Dashboard and BI tools. The workflow of the first use case is depicted in Figure \ref{fig:intro}. The second use case is related to machine learning or collaborative filtering. A workflow in this use case consists of the followings steps: (1) load the logs and database backups into the HDFS, (2) perform the collaborative filtering and machine learning computation, (3) produce the production datasets in Hadoop, for example, (4) perform the sanity check of the production data set, and (5) at the end, load the cleaned data to production data store \cref{crobak}.

\begin{figure}[here]
\centering
\includegraphics[width=1.0\linewidth]{figures/intro}
\caption{Workflow for Analytics/Data Warehousing \cref{crobak}}
\label{fig:intro}
\end{figure}

\section{Motivation}
The current existing WMS, which will be explored further in section \ref{sec:rw}, mainly act as a "glue" of simple jobs defined by the developer. Data dependencies and control flow in the workflow (e.g. decision making, looping, and branching) are specified manually in the model. This manual specification of dependencies causes a large overhead to the programmer. Thus, it would be convenient if there exist a design of a WMS that is able to automatically detect the control flow and data dependencies between the tasks based on pure program code that is written in a specific language familiar to the programmer. 

The overall goal of this thesis is to design and preliminary develop a WMS prototype that works on top of Stratosphere \cite{alexandrov2014stratosphere}, an emerging large-scale data processing framework developed by TU Berlin, Humboldt Universit\"{a}t zu Berlin, and the Hasso-Plattner-Institut funded by the Deutsche Forschungsgemeinschaft (DFG). The approach that will be taken to define the workflow specification is to develop a Domain Specific Language (DSL) \cite{van2000domain} on top of Scala \cite{odersky2004overview} high-level programming language. The idea is to build a WMS that will take as an input a program, in which the programmer defines a set of tasks associated with each other in a given sequence in a high-level language that is fairly similar to Scala, only with a set of restrictions. Control flow and data dependencies will be automatically detected by static analysis on the program code. The WMS will subsequently execute the tasks in a sequence that is according to the control flow and data dependencies. Language integration has been an old goal in the database community. We would like to query, manipulate, store and process data in the same language.

The first step to building this ideal WMS is to develop the programming model for the workflow DSL. In principle, a workflow language is required to have a model to maintain the tasks and the relationships between the tasks, both control flow and data dependencies. We will define the grammar of DSL to guide the programmer to write a workflow specification in our DSL. This workflow specification will be taken by the WMS as the input program. After the workflow specification is defined in the input program, the process of compiling the program into the target code to be run in the Stratosphere is divided into three stages as follows: (1) conversion from input program to an intermediate representation (IR) in a form of a control flow graph whereby the nodes of the graph represent tasks that can be run at once, (2) enrich the control flow graph with data dependencies between the nodes of the graph by performing data flow analysis, and (3) conversion of the IR which is the control-flow-enriched data flow to the script of the jobs. We will define a language grammar for this Scala DSL. This grammar defines the scope of Scala grammar \cite{odersky2004scala} that can be understood, analyzed and later processed by our DSL to generate the IR and final job scripts to be run in the WMS. The overall framework of our DSL, compiler, and WMS is depicted in Figure \ref{fig:framework}. With regards to these stages of development, our contribution in this thesis is summarized as follow:
\begin{itemize}
\item define the DSL grammar and programming model for our workflow,
\item analyze the user program to produce an IR in the form of a control flow graph, and 
\item present an algorithm to detect data dependencies between each node of the graph as well as an algorithm to generate the job scripts for the target machine with regards to the second and third stage of the WMS development.
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{figures/compframework}
\caption{Overall Framework for DSL, Compiler, and WMS}
\label{fig:framework}
\end{figure}

The most important aim of this process, as mentioned in the beginning, is to avoid the manual job of defining dependencies,  both tasks and data, when building the workflow. In an Oozie \footnote{\label{oozie}http://oozie.apache.org} workflow, an existing workflow system for Hadoop, nodes in the DAG are forward-chain, that is, the programmer is required to specify manually where a node or a computation in the DAG goes after it finishes. This can be hard to track and it forces the developer to remember every node in the chain when developing the workflow \footnote{\label{scoozie}https://github.com/klout/scoozie}. Thus, the Scala DSL that we aim to develop will focus around dependencies. The developer needs to look at one node in a workflow at a time, but does not need to define the tasks that that node depends on, the dependencies, both control flow and data dependencies, will be discovered by Scala code analysis. In performing the code analysis, we identify the self-contained jobs within branches of the Abstract Syntax Tree generated by the Scala compiler before finally generate the code for the underlying system. 

In the evaluation (Chapter \ref{chap:eval}), we argue over the advantages of this DSL compared to related WMS work in terms of productivity and independence of underlying platform by selecting a use case that is representative of use cases running on large-scale data processing platform. We show that even though currently our workflow DSL can only run on Stratosphere, it can easily be extended to be used on another system. 

\begin{definition}
A domain specific language (DSL) is "a programming language or executable specification language that offers, through appropriate notations and abstractions, expressive power focused on, and usually restricted to, a particular problem domain". \cite{van2000domain}
\label{def:dsl}
\end{definition}

\section{Thesis Outline}
\textbf{Chapter 2} contains the background and related work on workflow systems and data flow systems. When presenting the literature survey, we also introduce the approach of our workflow model and workflow language.\\
\textbf{Chapter 3} presents the implementation part of our DSL, compiler stages, and WMS. We define the grammar of our workflow language and talks about the process of transforming the user program to the target code for the underlying system through the three stages.\\
\textbf{Chapter 4} discusses about the advantages of our workflow DSL in terms of ease of use and extensible to other platform by means of implementing a sample use case.\\
\textbf{Chapter 5} is the conclusion in which we summarize our contribution throughout this research and present the limitations and potential future works. 