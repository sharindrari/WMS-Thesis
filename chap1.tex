%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}
With the advancement of Big Data Analytics, numerous systems for cluster computing on big data have been developed \cite{zaharia2012resilient,battre2010nephele,thusoo2010hive,dean2008mapreduce,yu2008dryadlinq,olston2008pig} and data engineers are building more and more complex applications to manage and process large data sets on distributed resources. Such complex application scenarios require means in order to compose and execute complex workflows. Workflows automate procedures that users would otherwise need to carry out manually \cite{deelman2009workflows}. It refers to a sequence of steps or computations that a user would like to perform 
\footnote{\label{crobak}http://www.crobak.org/2012/07/workflow-engines-for-hadoop}. As an example, within a Hadoop \footnote{\label{hadoop}http://hadoop.apache.org/.} cluster, a user may need to export the production databases and load the data to the Hadoop File System (HDFS) as the first step. The second step would be to run a MapReduce job to clean up the data and step three would be a set of operations that run in parallel to count and filter the data. A workflow is intended to map all of the different operations together. Such a workflow is usually represented as a Directed Acyclic Graph (DAG) where the nodes can be tasks or control flow structures and edges represent the relationships between tasks, namely task or data dependency. A Workflow Management System (WMS) is a system that allows users and developers to create, define, run, and delete a workflow \cref{crobak}. 
	
As an introduction to what a workflow may look like, we will walk through two different use cases that are representative use cases in the Big Data environment. The first use case is Analytics/Data Warehousing. A workflow in this first use case consists of the following steps: (1) load the logs into the Fact tables, (2) load the database backups into the Dimension tables, (3) compute the aggregations and perform rollups/cubes inside Hadoop for instance, (4) load the data into a low latency store, and (4) in the end, perform the analytics using a Dashboard and BI tools. The second use case is related to machine learning or collaborative filtering. A workflow in this use case consists of the followings steps: (1) load the logs and database backups into the HDFS, (2) perform the collaborative filtering and machine learning computation, (3) produce the production datasets in Hadoop, for example, (4) perform the sanity check of the production data set, and (5) at the end, load the cleaned data to production data store \cref{crobak}.

\begin{figure}[here]
\centering
\includegraphics[width=1.0\linewidth]{figures/intro}
\caption{Workflow for Analytics/Data Warehousing \cref{crobak}}
\label{fig:intro}
\end{figure}

\section{Motivation}
The current existing WMS, which will be explored further in Section, mainly act as a "glue" of simple jobs defined by the developer. Data dependencies and control flow in the workflow (e.g. decision making, looping, and branching) are specified manually in the model. This manual specification of causes a large overhead and confusion to the programmer. Thus, it would be convenient if there exist a design of a WMS that is able to automatically detect the control flow and data dependencies between the tasks based on pure program code that is written in a specific language familiar to the programmer. 

The overall goal of this thesis is to design and partly develop a WMS prototype that works on top of Stratosphere \cite{alexandrov2014stratosphere}, an emerging large-scale data processing framework developed TU Berlin, Humboldt Universit\"{a}t zu Berlin, and the Hasso-Plattner-Institut funded by the Deutsche Forschungsgemeinschaft (DFG). The approach that will be taken to define the workflow specification is to develop a Domain Specific Language (DSL) \cite{van2000domain} on top of Scala \cite{odersky2004overview} high-level functional programming language. The idea is to build a WMS that will take as an input a program, in which the programmer defines a set of tasks associated with each other in a given sequence in a high-level language that is fairly similar to Scala functional programming language, only with a set of restrictions. The WMS and then execute the tasks. Control flow, and data dependencies will be automatically detected by static analysis on the program code. The WMS will subsequently execute the tasks in a sequence that is according to the control flow and data dependencies. Language integration has been an old goal in the database community. We would like to query, manipulate, store and process data in the same language. That said, the tasks that are triggered by the workflow can be written in any other language e.g. Java. 

The first step to building this ideal WMS is to develop the programming model for the workflow DSL. In principle, a workflow language is required to have a model to maintain the tasks and the relationships between the tasks, both control flow and data dependencies. We will define the grammar of DSL to guide the programmer to write a workflow specification in our DSL. This workflow specification will be taken by the WMS as the input program. After the workflow specification is defined in the input program, the process of compiling the program into the target code to be run in the Stratosphere is divided into three stages as follows: (1) conversion from input program to an intermediate representation in a form of a control flow graph whereby the nodes of the graph represent tasks that can be run at once, (2) enrich the control flow graph with data dependencies between the nodes of the graph by performing data flow analysis, and (3) conversion of the intermediate representation which is the control flow enriched dataflow to the script of the jobs \footnote{\label{scoozie}https://github.com/klout/scoozie}. We will define a language grammar for this Scala DSL. This grammar defines the scope of Scala grammar \cite{odersky2004scala} that can be understood, analyzed and later processed by our language  to generate the intermediate representation and final job scripts to be run in the WMS. With regards to these stages of development, our contribution in this thesis is summarized as follow:
\begin{itemize}
\item Define the DSL grammar and programming model for our workflow,
\item Analyze the user program to produce an intermediate representation in the form of a control flow graph, and 
\item With regards to the third and fourth stage of the WMS development, we present an algorithm to detect data dependencies between each node of the graph as well as an algorithm to generate the job scripts for the target machine. 
\end{itemize}

The most important aim of this process, as mentioned in the beginning, is to avoid the manual job of defining dependencies,  both tasks and data, when building the workflow. In a Oozie \footnote{\label{oozie}http://oozie.apache.org} workflow, an example of workflow systems for Hadoop, nodes in the DAG are forward-chain, that is, the developer needs to specify where a node or a computation in the DAG goes after it is finished. This can be hard to track and it requires the developer to remember every node in the chain when developing the workflow \cref{scoozie}. Thus, the Scala DSL that we aim to develop will attempt to focus around dependencies. The developer needs to look at one node in a workflow at a time, but does not need to define the tasks that that node depends on, the dependencies will be discovered by Scala code analysis. The approach that we will be taking to perform the code analysis and code generation would be to identify the self-contained jobs within branches of the Abstract Syntax Tree generated by the Scala compiler. 

In the evaluation section, we argue over the advantages of this DSL compared to related WMS work in terms of user-friendliness and independence of underlying platform by selecting a use case that is representative of use cases running on Stratosphere. We show that even though at the moment this DSL can only run on Stratosphere, it can be extended to be used on another system. 

\begin{definition}
A domain-specific language (DSL) is a program- ming language or executable specification language that offers, through appropriate notations and ab- stractions, expressive power focused on, and usu- ally restricted to, a particular problem domain \cite{van2000domain}
\label{def:dsl}
\end{definition}

\section{Thesis Outline}
\textbf{Chapter 2} contains the background and related work on workflow systems and dataflow systems. When presenting the literature survey, we also introduce the approach of our workflow model.\\
\textbf{Chapter 3} presents the implementation part of our DSL and WMS. We define the grammar of our workflow language and talks about the process of transforming the user program to an an intermediate representation which is an control flow enriched dataflow.\\
\textbf{Chapter 4} discusses about the advantages of using our workflow language in terms of ease of use and extensible to any platform.\\
\textbf{Chapter 5} is the conclusion in which we summarize our contribution and present the limitations and potential future work. 