%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}
With the advancement of Big Data Analytics, data engineers are building more and more complex applications to manage and process large data sets on distributed resources. Such complex application scenarios require means in order to compose and execute complex workflows. Workflows automate procedures that users would otherwise need to carry out manually \cite{deelman2009workflows}. A workflow refers to a sequence of steps or computations that a user would like to perform 
\footnote{\label{crobak}http://www.crobak.org/2012/07/workflow-engines-for-hadoop}. As an example, within a Hadoop \footnote{\label{hadoop}http://hadoop.apache.org/.} cluster, a user may need to export the production databases and load the data to the Hadoop File System (HDFS) as the first step. The second step would be to run a MapReduce job to clean up the data and step three would be a set of operations that run in parallel to count and filter the data. A workflow is intended to map all of the different operations together. Such a workflow is usually represented as a Directed Acyclic Graph (DAG) where the nodes can be tasks or control flow structures and edges represent the relationships between tasks, namely task or data dependency. A Workflow Management System (WMS) is a system that allows users and developers to create, define, run, and delete a workflow \cref{crobak}. 
	
As an introduction to what a workflow may look like, we will walk through two different use cases that are representative use cases in the Big Data environment. The first use case is Analytics/Data Warehousing. A workflow in this first use case consists of the following steps: (1) load the logs into the Fact tables, (2) load the database backups into the Dimension tables, (3) compute the aggregations and perform rollups/cubes inside Hadoop for instance, (4) load the data into a low latency store, and (4) in the end, perform the analytics using a Dashboard and BI tools. The second use case is related to machine learning or collaborative filtering. A workflow in this use case consists of the followings steps: (1) load the logs and database backups into the HDFS, (2) perform the collaborative filtering and machine learning computation, (3) produce the production datasets in Hadoop, for example, (4) perform the sanity check of the production data set, and (5) at the end, load the cleaned data to production data store \cref{crobak}.

\section{Overview}
The current existing WMS, which will be explored further in the related work section, mainly act as a “glue” of simple jobs defined by the developer. Data dependencies and control flow in the workflow (e.g. decision making, looping, and branching) are specified manually in the model. This manual job causes a large overhead and confusion to the developer. It would be convenient if the WMS is able to automatically detect the control flow and data dependencies between the tasks based on pure program code. 

The overall goal of this thesis is to design and develop a prototype of WMS that works on top of Stratosphere, the Big Data Analytics platform developed by DIMA in TU Berlin. The approach that will be taken to define the workflow specification is to develop a Domain Specific Language (DSL) on top of Scala \cite{odersky2004overview}, a high-level functional programming language. The idea is to build a WMS that will take a Scala program which defines a set of tasks associated with each other in a given sequence, and then execute the tasks. Control flow, and data dependencies will be automatically detected by static analysis on the Scala code. However, the tasks that are triggered by the workflow can be written in any other language e.g Java. Language integration has been an old goal in the database community. We would like to query, manipulate, store and process data in the same language. 

The first goal of building a WMS is to develop the programming model for the Scala DSL. In principle, a WMS has a model to maintain the relationships between the processes, tasks, and the various states. Thus, the deliverables of this step are the layers of the workflow; (1) specification of job in DSL, (2) conversion from DSL to an intermediate representation which takes the form of a control flow graph, (3) generate the data dependencies between the nodes in the graph, and (4) conversion of the intermediate representation to the script of the jobs \footnote{\label{scoozie}https://github.com/klout/scoozie}. We will define a language grammar for this Scala DSL. This grammar defines the scope of Scala grammar \cite{odersky2004scala} that can be understood, analyzed and later processed by our language  to generate the intermediate representation and final job scripts to be run in the WMS. With regards to these stages of development, our contribution in this thesis is summarized as follow:
\begin{itemize}
\item Defining the DSL grammar and programming model
\item Analyze the program to produce an intermediate representation in the form of a control flow graph. 
\item With regards to the third and fourth stage of the WMS development, we present an algorithm to detect data dependencies between each node of the graph as well as an algorithm to generate the job scripts for the target machine. 
\end{itemize}

The most important aim of this process, as mentioned in the beginning, is to avoid the manual job of defining dependencies,  both tasks and data, when building the workflow. In a Oozie \footnote{\label{oozie}http://oozie.apache.org} workflow, an example of workflow systems for Hadoop, nodes in the DAG are forward-chain, that is, the developer needs to specify where a node or a computation in the DAG goes after it is finished. This can be hard to track and it requires the developer to remember every node in the chain when developing the workflow \cref{scoozie}. Thus, the Scala DSL that we aim to develop will attempt to focus around dependencies. The developer needs to look at one node in a workflow at a time, but does not need to define the tasks that that node depends on, the dependencies will be discovered by Scala code analysis. The approach that we will be taking to perform the code analysis and code generation would be to identify the self-contained jobs within branches of the Abstract Syntax Tree generated by the Scala compiler. 

In the evaluation section, we argue over the advantages of this DSL compared to related WMS work in terms of user-friendliness and independence of underlying platform by selecting a use case that is representative of use cases running on Stratosphere. We show that even though at the moment this DSL can only run on Stratosphere, it can be extended to be used on another system. 