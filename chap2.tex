%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Background, Approach, and Related Work} 

The study and development of workflows and workflow management systems have been conducted for years and in various field (i.e. e-Science, Grid computing, and recently Big Data Analytics). In this chapter we present the background on workflow systems, workflow language, workflow model, as well as introducing the design and programming model approach for our workflow language. Furthermore, we also present some of the related work on workflow and dataflow systems especially those developed to run on top of large-scala data processing platform. 

\section{Workflow}

Workflow systems \cite{yu2005taxonomy,deelman2009workflows,von2007work,barker2008scientific} provide a way for programmers to arrange the tasks to be executed in a variety of different ways (A workflow is defined formally in Definition \ref{def:workflow}). A workflow is especially important for applications in which data dependencies exist between the tasks, and a flexible mechanism of arranging the tasks is necessary so that data produced by some tasks can be consumed by others \cite{kelly2011applying}. 
In order to execute such a workflow, we require a Workflow Management System (WMS) \cite{von2007work} (refer to Definition \ref{def:wms} for a formal definition of WMS). Workflow languages refer to a form of high-level programming language in which operations correspond to tasks that are executed by external pieces of software, usually remotely \cite{kelly2011applying}. Commonly used design patterns in workflows have been surveyed by van der Aalst et. al. \cite{van2003workflow}, Bharathi et. al. \cite{bharathi2008characterization}, Pautasso and Alonso \cite{pautasso2006parallel}, and Yildiz, Guabtni, and Ngu \cite{yildiz2009towards}.

\begin{definition}
"Workflow is the computerised facilitation or automation of a business process, in whole or part \cite{hollingsworth1993workflow}."
\label{def:workflow}
\end{definition}

\begin{definition}
"Workflow Management System (WMS) is a system that completely defines, manages and executes “work- flows” through the execution whose order of execution is driven by a computer representation of the workflow logic \cite{hollingsworth1993workflow}."
\label{def:wms}
\end{definition}

\subsection{Workflow Design}
\cite{yu2005taxonomy} classifies the workflow design to at least three taxonomies, namely: (a) workflow structure, (b) workflow model/specification, and (c) workflow composition system. In the view of workflow structure, a workflow is composed by connecting multiple tasks according to their dependencies. In general, the workflow can be represented as a DAG or non-DAG. The workflow that we develop in this thesis belongs to non-DAG-based workflow. In a non-DAG workflow, workflow structure is categorized into sequence, parallelism, choice, and iteration. Sequence is defined as an ordered series of tasks, with one task starting after a previous task has completed. Parallelism represents tasks which are performed concurrently, rather than serially. In choice control pattern, a task is selected to execute at run-time when its associated conditions are true. In Iteration structure, also known as loop, sections of workflow tasks in an iteration block are allowed to be repeated and is often occurred in workflow of complex use cases \cite{yu2005taxonomy}.  

Workflow Model, which is also called workflow specification, defines a workflow including its task definition and structure definition. There are two types of workflow models, namely abstract model and concrete model, denoted as abstract workflow and concrete workflow, respectively \cite{deelman2004workflow}. In the abstract model, a workflow is described in an abstract form, in which the workflow is specified without referring to specific resources for task execution. The abstract model enables users to define workflows without being concerned about low-level implementation details. In contrast, the concrete model binds workflow tasks to specific resources \cite{yu2005taxonomy}. In this thesis, we implement the abstract workflow which takes the form of control flow enriched dataflow. Furthermore, we also provide the algorithm to achieve the concrete workflow, namely the job scripts to be executed in the underlying system. 

\begin{figure}[here]
\centering
\includegraphics[width=1.0\linewidth]{figures/taxonomy}
\caption{Thesis Workflow Design Approach}
\label{fig:Taxonomy}
\end{figure}

Workflow composition systems are designed for enabling users to assemble components into workflows \cite{yu2005taxonomy}. It consists of two classes: (1) User-directed and (2) automatic.  User-directed composition systems allow users to edit workflows directly, whereas automatic composition systems generate workflows for users automatically \cite{yu2005taxonomy}. Both classes are implemented in this thesis. More specifically, in the user-directed, language-based modeling is applied since we require user to write their program in our Scala DSL. However, the control flow and data dependencies will later be identified automatically by performing the static analysis of the Scala program. In this case, user does not have to specify manually the workflow components and dependencies. To summarize, the taxonomy of workflow that we develop in this thesis is depicted in Figure \ref{fig:Taxonomy}.

\subsection{Workflow Language}
\cite{kelly2011applying} well summarizes the common concepts of workflow language. Some of those concepts compose the programming model of our workflow language. We define these concepts and map them in high-level to our programming model as follows:
\begin{itemize}
\item \textbf{Tasks} refer to units of work that are used within a workflow. Each task takes as input a set of values, and produces another set of values as output when executed. A task can thus be considered equivalent to a function. Each task in our abstract workflow corresponds to a node in our control flow enriched data flow graph that we will explain in detail in Chapter 3. Whereas in our concrete workflow, a task correspond to one job that will be executed in the underlying system. 
\item \textbf{Data dependencies} are directional relationships between tasks, each indicating that the input of one task is an output of another. These data dependencies determine which data to supply as input when executing a task as well as the relative order of the execution of the tasks. In our abstract workflow, these data dependencies correspond to directional data-flow edges in the graph. 
\item \textbf{Control dependencies} indicate a constraint on the relative order of execution of two tasks, without implying data transfer. They only occur in tasks with special effects, where data dependencies alone are not sufficient to determine the ordering.
\item \textbf{Parallelism} is enabled by establishing the dependencies in the structure of the workflow. Any set of tasks that are not dependent on each other can be run in parallel. The WMS uses the control and data dependencies to automatically determine which set of tasks to run in parallel. 
\item \textbf{Conditional branching and iterations} in a workflow language is similar to branching and iterations in a conventional programming language. Based on a certain condition, a conditional branch selects which part of the workflow will be subsequently executed. The condition itself is performed by executing a task with some input data and returns a boolean value. Iteration enables part of the workflow to be run multiple times, such that in each iteration, tasks withing the loop body are executed with different input value, given that the condition of the iteration is satisfied. Our workflow language support both conditional branching and iterations. 
\item \textbf{Representation} of a workflow normally consists of a graph, whereby nodes corresponds to tasks and edges correspond to dependencies. Parallel execution uses the dependency information to determine which tasks can be run in parallel. In chapter 3, we will see in detail the representation of our workflow language. 
\end{itemize} 

\section{Control Flow vs Data Flow}
In addition to the three taxonomies described in the previous section, most, if not all, workflow design belongs to one of two classes: control flow or data flow. The two classes are similar in that they specify the interaction between individual tasks within the group that comprise the workflow. The difference between the two is in their methods of implementing that interaction. In control-driven workflows, or control flows, the connections between the activities in a workflow represent a transfer of control from the preceding task to successor task. This includes control structures such as sequences, conditionals, and iterations. Data-driven workflows, or data flows, on the other hand, are designed to support data-driven applications. The dependencies represent the flow of data between workflow activities from data producer to data consumer \cite{deelman2009workflows}. More details explanation on control flow and data flows are presented in the following sections. 

\subsection{Control Flow Model}\label{ch1:opts}
Most control flow languages provide support not only for simple flows of control between components or services in the workflow but also for more complex control interactions such as iterations and conditionals branching. Users of workflow systems often require more than the simple control constructs that are available to them. The ability to perform branching in the workflow based on conditions and loop over sections of the workflow repeatedly is important for all applications especially for complex use cases \cite{deelman2009workflows}. The important issue here is how to represent these conditionals and iterations in the workflow language and to what degree the language should support them. For instance, there is a question to whether a single simple loop construct is  enough, or whether the language should support all loop types(i.e. while, do while, for). In the case of conditional behavior, the problem is determining whether the incoming value and the conditional value are equivalent \cite{deelman2009workflows}.

The term control flow refer to the parts of a program that determine what computation is to be performed, based on various conditions \cite{kelly2011applying}. Examples of control flow constructs include conditional statements, iteration, function calls, and recursion. Control flow does not necessarily dictate the exact order in which execution will occur, but rather determines which expressions will be evaluated in order to compute the final result of a program. Most workflow languages provide limited forms of control flow, such as conditional statements and sequential iteration.

Standard control flow constructs such as conditionals branching and iterations are supported by most large-scale data processing systems such as Stratosphere \cite{alexandrov2011mapreduce} and Spark \cite{zaharia2010spark}. In Stratosphere and Spark API, for example, expressions can include standard arithmetic, relational, and boolean operators, as well as their primitive operators such as map, reduce, filter, etc. The result of an expression may be used in any place that a regular value may be, such as input to a conditional test for branching or iteration. Below is the example of expression in a condition test that can be understood by Stratosphere and Spark API for branching and iteration, respectively.
\begin{lstlisting}[language=scala, label = controlflow]
if (A.reduce(_ + _)
while (B.reduce(_ + _) < 10000)
\end{lstlisting} 

\subsection{Dataflow Model}

In a dataflow model, a workflow is represented as a task dependency graph, in which nodes correspond to tasks, and edges indicate data dependencies. The graph specifies a relative order of execution, by constraining each task to begin only after those it depends on have completed assuming that each task is free of side effects, and that its output depends only on its inputs \cite{kelly2011applying}. Most data flow representations are very simple in nature, and unlike their control flow counterparts, contain nothing apart from component or service descriptions and the data dependencies between them; control constructs such as loops and branching are generally not included. The dependencies between tasks are data dependencies, ensuring the data producer has finished before the consumer may start \cite{deelman2009workflows}. The key advantage is that, in dataflow, more than one set of tasks can be executed at once. Tasks with no dependencies between them may thus be safely executed in parallel. This simple principle provides the potential for massive parallel execution at the tasks level \cite{johnston2004advances}.

A dataflow in Stratosphere is a directed acyclic graph (DAG) that consists of operators, data sources, and sinks or output. The data sources and sinks, as well as the intermediate data sets that flow through operators, are bags of records. A dataflow construct of several operators is therefore a function, whose fixpoint we can find by terminating the loop in the dataflow DAG \cite{ewen2012spinning}. Spark has a feature called Resilient Distributed Datasets (RDDs), a distributed memory abstraction that enables programmers perform in-memory computations on large clusters in a fault-tolerant manner\cite{zaharia2012resilient}.

\section{Related Work}
There are some major existing data flow systems and workflow systems that are interesting to mention. This section presents these different dataflow and workflow systems and discuss their characteristics, advantages, and limitations. 

\subsection{Dataflow Systems}
Pig is a procedural dataflow system for MapReduce. It offers a SQL-style high-level data manipulation constructs, which can be assembled in any explicit dataflow and combine with custom MapReduce style functions. Pig programs are compiled into sequences of MapReduce jobs and run on the Hadoop MapReduce environment \cite{gates2009building}. Pig write jobs for the Hadoop platform using a DSL called Pig Latin \cite{olston2008pig}. It is a dataflow language that provides extensive support for relational operations. Pig Latin users need to learn a new language which is not the case with frameworks such as Hadoop. It does not include user defined functions, the user needs to define them externally in another language, which will often prevent optimizations as Pig can not analyze those \cite{ackermann2012jet}. 

A dataflow system developed at Microsoft is Naiad \cite{murray2013naiad} which offers a new computational model called timely dataflow. This timely dataflow model enriches dataflow computation with timestamps that represent logical points in the computation and provide the basis for an efficient coordination mechanism. The model is based on a directed graph in which the nodes send and receive logically timestamped messages along directed edges \cite{murray2013naiad}. 

Many existing systems for cluster computing have been developed in the past decade namely Hadoop MapReduce, Dryad \cite{isard2007dryad}, Pig \cite{olston2008pig}, Hive \cite{thusoo2010hive} and Spark \cite{zaharia2010spark}. Hadoop MapReduce and Dryad allow programmer to write low level optimized code but can greatly reduce programmer’s productivity. On the other hand, Spark provides high level operations to increase productivity but hard to gain performance by doing relational op-
timization. Other systems like Pig and Hive, have a very limited interface to solve a wide range of problems. Programmers are often required to write their own functions which are not convenient and hard to optimize \cite{ackermann2012jet}. Jet is a new domain specific framework which provides high level abstraction similar to Spark and performs relational optimization as well as domain-specific optimizations. It is built to construct the intermediate representation of program and generates optimized code for Spark and Crunch \footnote{https://github.com/cloudera/crunch.,} which is comparable with hand optimized implementation \cite{ackermann2012jet}. 

\subsection{Workflow Systems}
In recent years, a number of WMSs have emerged in the Big Data community and they are developed to run on top of Hadoop and/or for more general purpose. Within the Hadoop community, a WMS called Apache Oozie is developed to enable user to combine multiple MapReduce jobs into a logical unit of work to accomplish larger tasks or a workflow \cite{islam2012oozie}. Oozie is a Java Web Application that stores the workflow definitions and the currently running workflow instances, including their status (e.g. running, stalled, failed) and variables (e.g. input files, output files). An Oozie workflow is a sequence of actions (e.g. Hadoop MapReduce jobs, Pig jobs) represented in a control dependency DAG that is specified in the XML Process Definition Language. The workflow consists of Control Nodes and Action Nodes. Control nodes define the flow of execution and include start and end node of a workflow as well as the mechanisms to control the workflow execution path e.g. decision, fork, and join nodes whereas action Nodes are the mechanism to allow a workflow trigger the execution of a processing task \cite{islam2012oozie}. 

Another example of a WMS that is not built specifically for Hadoop is Luigi and is developed by Spotify. Luigi is a Python package that helps developers build complex pipelines of batch jobs \footnote{\label{luigi}https://github.com/spotify/luigi}. It facilitates developers to combine many tasks together, where each task may be a Hadoop job, a Hive query, loading a table from a database, etc. Luigi takes care of large portion of the workflow management so that the developer can focus on the tasks themselves and their dependencies. One major difference between Luigi and Oozie is that instead of XML configuration, the DAG in Luigi is specified with Python code constructs. This makes it easy to build complex dependency graphs of tasks. Additionally, the workflow can trigger scripts that are not written in Python e.g., Pig scripts \cref{luigi}. The Luigi WMS consists of a centralized scheduler and a number of workers. The workers communicate with the scheduler over a JSON REST API. In Luigi, the developer defines a job as a Python class. Luigi workers communicate with HDFS and walk through the work-flow DAG and check, for each task, whether a task’s output exists in order to determine the next tasks to be run. After it walks through the DAG, it then runs the tasks e.g. MapReduce jobs \cref{crobak}. 