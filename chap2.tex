%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Background, Approach, and Related Work}
\label{chap:background}

The study and development of workflows and WMS have been conducted for years and in various field (i.e. e-Science, Grid computing, and recently Big Data Analytics). In this chapter, we present the background on workflow systems which includes workflow design and workflow language, as well as introducing the design and programming model approach for our workflow language. Furthermore, we also present some of the related work on workflow and data flow systems especially those developed to run on top of large-scala data processing platforms. 

\section{Workflow}\label{sec:workflow}

Workflow systems \cite{yu2005taxonomy,deelman2009workflows,von2007work,barker2008scientific} provide a way for programmers to arrange the tasks to be executed in a variety of different ways (a workflow is defined formally in Definition \ref{def:workflow}). A workflow is especially important for applications in which data dependencies exist between the tasks. A flexible mechanism of arranging the tasks is necessary so that data produced by some tasks can be consumed by others \cite{kelly2011applying}. 
In order to execute such a workflow, we require a WMS \cite{von2007work} (refer to Definition \ref{def:wms} for a formal definition of WMS). Workflow languages are some kind of meta programming language in which operations correspond to tasks that are executed by external pieces of software, usually remotely \cite{kelly2011applying}. Commonly used design patterns in workflows have been surveyed by van der Aalst et. al. \cite{van2003workflow}, Bharathi et. al. \cite{bharathi2008characterization}, Pautasso and Alonso \cite{pautasso2006parallel}, and Yildiz, Guabtni, and Ngu \cite{yildiz2009towards}.

\begin{definition}
Workflow is "the computerized facilitation or automation of a business process, in whole or part" \cite{hollingsworth1993workflow}.
\label{def:workflow}
\end{definition}

\begin{definition}
Workflow Management System is "a system that completely defines, manages and executes “workflows” through the execution whose order of execution is driven by a computer representation of the workflow logic" \cite{hollingsworth1993workflow}.
\label{def:wms}
\end{definition}

\subsection{Workflow Design}\label{sec:WFDesign}
\cite{yu2005taxonomy} classifies the design of a workflow to at least three taxonomies, namely: (a) workflow structure, (b) workflow model/specification, and (c) workflow composition system. In the view of workflow structure, a workflow is composed by connecting multiple tasks according to their dependencies. In general, the workflow may be represented as a DAG or non-DAG. The workflow that we develop in this thesis classifies into non-DAG-based workflow. In a non-DAG workflow, workflow structure is categorized into sequence, parallelism, choice, and iteration. Sequence is defined as an ordered series of tasks, with one task starting after a previous task has completed. Parallelism represents tasks which are performed concurrently, rather than serially. In choice control pattern, a task is selected to execute at runtime when its associated conditions are true. In iteration structure, also known as loop, sections of workflow tasks in an iteration block are allowed to be repeated. Iteration is often occurred in workflow of complex use cases \cite{yu2005taxonomy}. 

Workflow Model, or workflow specification, defines a workflow in terms of its task definition and structure definition. There are two types of workflow models, namely abstract model and concrete model, denoted as abstract workflow and concrete workflow, respectively \cite{deelman2004workflow}. In the abstract model, a workflow is described in an abstract form, in which the workflow is specified without referring to specific resources for task execution. The abstract model enables users to define workflows without being concerned about low-level implementation details. In contrast, the concrete model binds workflow tasks to specific resources \cite{yu2005taxonomy}. In this thesis, we implement the abstract workflow in a form of control flow enriched data flow. Furthermore, we also provide the algorithm to achieve the concrete workflow, namely the job scripts to be executed in the underlying system. The representation of abstract workflow and concrete workflow of our design will be presented in details in Chapter 3. 

\begin{figure}[here]
\centering
\includegraphics[width=1.0\linewidth]{figures/taxonomy}
\caption{Thesis Workflow Design Approach}
\label{fig:Taxonomy}
\end{figure}

Workflow composition systems are designed for enabling users to assemble components into workflows \cite{yu2005taxonomy}. It consists of two classes: (1) user-directed and (2) automatic.  User-directed composition systems allow users to edit workflows directly, whereas automatic composition systems generate workflows for users automatically \cite{yu2005taxonomy}. Both classes are adopted in this thesis. More specifically, in the user-directed, language-based modeling is applied since we require the users to write their program in our workflow DSL. However, the control flow and data dependencies will later be identified automatically by performing the static analysis of the user program. In this case, users are not required to specify manually the workflow components and dependencies. In summary, the taxonomy of workflow that we develop in this thesis is depicted in Figure \ref{fig:Taxonomy}.

\subsection{Workflow Language}\label{sec:WFLanguage}
\cite{kelly2011applying} well summarizes the common concepts of workflow language. Some of those concepts compose the programming model of our workflow language. We describes these concepts and map them in a high-level manner to our programming model in the following points:
\begin{itemize}
\item \textbf{Tasks} refer to units of work that are used within a workflow. Each task takes as input a set of values, and produces another set of values as output when executed. A task can thus be considered equivalent to a function. Each task in our abstract workflow corresponds to a node in our control-flow-enriched data flow. Whereas, in our concrete workflow, a task corresponds to one job that will be executed in the underlying system. 
\item \textbf{Data dependencies} are directional relationships between tasks, each indicating that the input of one task is an output of another. These data dependencies determine which data to supply as input when executing a task as well as the relative order of the execution of the tasks. In our abstract workflow, these data dependencies correspond to directional data flow edges in the graph. 
\item \textbf{Control dependencies} indicate a constraint on the relative order of execution of two tasks, without implying data transfer. They only occur in tasks with special effects, whereby data dependencies alone are not sufficient to determine the ordering.
\item \textbf{Parallelism} is enabled by establishing the dependencies in the structure of the workflow. Any set of tasks that are not dependent on each other can be executed in parallel. The WMS uses the control and data dependencies to automatically determine which set of tasks to run in parallel. 
\item \textbf{Conditional branching and iterations} in a workflow language is similar to branching and iterations in a conventional programming language. Based on a certain condition, a conditional branch selects which part of the workflow will be subsequently executed. The condition itself is performed by executing a task with some input data and returns a boolean value. Iteration enables part of the workflow to be run multiple times, such that in each iteration, tasks within the loop body are executed with different input value, given that the condition of the iteration is satisfied. Our workflow language supports both conditional branching and iterations. 
\item \textbf{Representation} of a workflow normally consists of a graph, whereby nodes correspond to tasks and edges to dependencies. Parallel execution uses the dependency information to determine which tasks can be executed in parallel. The representation of our workflow language will be presented in detail in Chapter 3. 
\end{itemize} 

\section{Control Flow vs Data Flow}\label{sec:CFvsDF}
In addition to the three taxonomies described in section \ref{sec:WFDesign}, most, if not all, workflow designs belong to one of the two classes: control flow or data flow. The two classes are similar in that they specify the interaction between individual tasks in the workflow. The difference between the two classes lies in their method of implementing that interaction. In control-driven workflows, or control flows, the connections between the tasks in a workflow represent a transfer of control from the preceding task to successor task. This includes control structures such as sequences, conditionals branching, and iterations. Data-driven workflows, or data flows, on the other hand, are designed to support data-driven applications. The dependencies represent the flow of data between workflow activities from data producer to consumer \cite{deelman2009workflows}. More details explanation on control flow and data flows are presented in the following subsections. 

\subsection{Control Flow Model}\label{sec:CFModel}
Most control flow languages provide support not only for simple flows of control between components in the workflow but also for more complex control interactions such as iterations and conditionals branching. Users of workflow systems often require more than the simple control constructs that are available to them. The ability to perform branching in the workflow based on conditions and loop over sections of the workflow repeatedly is important for all applications especially for complex use cases \cite{deelman2009workflows}. The important issue here is how to represent these conditionals and iterations in the workflow language and to what degree the language should support them. For instance, there is a question to whether a single simple loop construct is  enough, or whether the language should support all loop types(i.e. while, do while, for). In the case of conditional behavior, the problem is determining whether the incoming value and the conditional value are equivalent or inequivalent (i.e. less than, greater than, etc) \cite{deelman2009workflows}.

The term control flow refers to parts of a program that determine what computation is to be performed, based on various conditions \cite{kelly2011applying}. Examples of control flow constructs include conditional statements, iteration, function calls, and recursion. Control flow does not necessarily dictate the exact order in which execution will occur, but rather determines which expressions will be evaluated in order to compute the final result of a program. Most workflow languages provide limited forms of control flow, such as conditional statements and sequential iteration.

Standard control flow constructs such as conditionals branching and iterations in a User Defined Function (UDF) are supported by most large-scale data processing systems such as Stratosphere \cite{alexandrov2011mapreduce} and Spark \cite{zaharia2010spark}. However, Stratosphere does not support conditionals branching on the outer level or outside UDFs which is the main motivation to introduce workflow on top of it. Spark, on the other hand, also allows the user to define workflows with conditionals branching and iterations \cite{zaharia2012resilient}. In Spark API, for example, expressions can include standard arithmetic, relational, and boolean operators, as well as their primitive operators such as map, reduce, filter, etc. The result of an expression may be used in any place that a regular value may be, such as input to a conditional test for a branching or an iteration. Below is the example of expression in a condition test that can be understood by Spark API for branching and iteration, respectively.
\begin{lstlisting}[language=scala, label = controlflow]
if (A.reduce(_ + _) < 30)
while (B.reduce(_ + _) < 10000)
\end{lstlisting} 
\subsection{Data Flow Model}

In a data flow model, a workflow is represented by a task dependency graph, in which the nodes of the graph correspond to tasks and edges to data dependencies. The graph specifies a relative order of execution, by constraining each task to begin only after those it depends on have completed assuming that each task is free of side effects, and that its output depends only on its inputs \cite{kelly2011applying}. Most data flow representations are very simple in nature, and unlike their control flow counterparts, contain nothing apart from component descriptions and the data dependencies between them. Control constructs such as loops and branching are generally not included in a data flow. The dependencies between tasks are data dependencies that ensures the data producer has finished before the consumer may start \cite{deelman2009workflows}. The key advantage is that, in data flow, more than one set of tasks can be executed at once. Tasks with no dependencies between them may thus be safely executed in parallel. This simple principle provides the potential for massive parallel execution at the tasks level \cite{johnston2004advances}.

A data flow in Stratosphere is a DAG which consists of operators, data sources, and sinks or output of the program. The data sources and sinks, as well as the intermediate data sets that flow through operators, are bags of records. For workflow with iterations, a data flow construct of several operators is therefore a function, whose fixpoint we can find by terminating the loop in the data flow DAG \cite{ewen2012spinning}.

\section{Related Work}\label{sec:rw}
There are some major existing data flow systems and workflow systems which are interesting to mention. This section presents these different data flow and workflow systems and discuss their characteristics, advantages, and limitations. 

\subsection{Data Flow Systems}
Many existing systems for cluster computing have been developed in the past decade such as Hadoop MapReduce, Dryad \cite{isard2007dryad}, Pig \cite{olston2008pig}, Hive \cite{thusoo2010hive} and Spark \cite{zaharia2010spark}. Hadoop MapReduce and Dryad allow programmer to write low level optimized code but can greatly reduce programmer’s productivity. On the other hand, Spark provides high level operations to increase productivity but hard to gain performance by doing relational optimization \cite{ackermann2012jet}. 

Pig is a procedural data flow system for MapReduce. It offers a SQL-style high-level data manipulation constructs, which can be assembled in any explicit data flow and combine with custom MapReduce style functions. Pig programs are compiled into sequences of MapReduce jobs and run on the Hadoop MapReduce environment \cite{gates2009building}. Pig write jobs for the Hadoop platform using a DSL called Pig Latin \cite{olston2008pig}. It is a data flow language that provides extensive support for relational operations. Pig Latin users need to learn a new language which is not the case with frameworks such as Hadoop. It does not include user defined functions, the user is thus required to define them externally in another language, which will often prevent optimizations as Pig can not analyze those \cite{ackermann2012jet}. 

Hive, is a solution which is similar with Pig. It supports queries expressed in a declarative language with the same feel as SQL - called HiveQL. These queries are compiled into MapReduce jobs and subsequently executed using Hadoop \cite{thusoo2010hive}. Both Hive and Pig, have a very limited interface to solve a wide range of problems. Programmers are often required to write their own functions which are not convenient and hard to optimize \cite{ackermann2012jet}. 

Jet is a new domain specific framework which provides high level abstraction similar to Spark and performs relational optimization as well as domain-specific optimizations. Its main aim is to construct the IR of program and generates optimized code for Spark and Crunch \footnote{https://github.com/cloudera/crunch} which is comparable with hand optimized implementation \cite{ackermann2012jet}. Jet is built upon the Lightweight Modular Staging (LMS) library \cite{rompf2010lightweight}. LMS uses facilities provided by Virtualized Scala, an experimental version of Scala which provides facilities for DSLs \cite{moors2012scala}, to build a modular compiler infrastructure for developing staged DSLs. 

Another existing data flow system which is developed at Microsoft is Naiad \cite{murray2013naiad}. It offers a new computational model called timely data flow whose aim is to enrich data flow computation with timestamps that represent logical points in the computation and provide the basis for an efficient coordination mechanism. The model is based on a directed graph in which the nodes send and receive logically timestamped messages along directed edges \cite{murray2013naiad}. 

\subsection{Workflow Systems}\label{sec:rwwf}
In recent years, a number of WMSs have emerged in the Big Data community and they are developed to run on top of Hadoop and/or for more general purpose. Within the Hadoop community, a WMS called Apache Oozie is developed to enable user to combine multiple MapReduce jobs into a logical unit of work to accomplish larger tasks or a workflow \cite{islam2012oozie}. Oozie is a Java Web Application that stores the workflow definitions and the currently running workflow instances, including their status (e.g. running, stalled, failed) and variables (e.g. input files, output files). An Oozie workflow is a sequence of actions (e.g. Hadoop MapReduce jobs, Pig jobs) represented in a control dependency DAG that is specified in the XML Process Definition Language. The workflow consists of Control Nodes and Action Nodes. Control nodes define the flow of execution and include start and end node of a workflow as well as the mechanisms to control the workflow execution path e.g. decision, fork, and join nodes whereas action Nodes are the mechanism to allow a workflow trigger the execution of a processing task \cite{islam2012oozie}. 

Another example of a WMS that is not built specifically for Hadoop is Luigi. Luigi is a Python package, built in Spotify, that helps developers build complex pipelines of batch jobs \footnote{\label{luigi}https://github.com/spotify/luigi}. It facilitates developers to combine many tasks together, where each task may be a Hadoop job, a Hive query, loading a table from a database, etc. Similar to Oozie, the developer need to define the tasks and their dependencies themselves. One major difference between Luigi and Oozie is that instead of XML configuration, the DAG in Luigi is specified with Python code constructs. This makes it easier to build complex dependency graphs of tasks. Additionally, the workflow can trigger scripts that are not written in Python e.g., Pig scripts \cref{luigi}. In Luigi, the developer defines a job as a Python class. Luigi workers communicate with HDFS and walk through the workflow DAG and perform data flow analysis. During the data flow analysis, Luigi checks, for each task, whether a task’s output exists in order to determine the next tasks to be run. After it finishes, it then runs the tasks e.g. MapReduce jobs \cref{crobak}. 

Spark \cite{zaharia2010spark}, as mentioned in section \ref{sec:CFModel}, supports generalized workflows. In contrast to MapReduce, where all jobs are expressed in rigid map and reduce semantics, Spark jobs can describe arbitrary workflows with one or multiple stages of processing \footnote{\label{mapr} http://www.mapr.com/products/product-overview/apache-spark}. This is enabled by advanced DAG engine feature that Spark has which supports cyclic data flow and in-memory computing. Each Spark job creates a DAG of task stages to be executed on the cluster \cite{zaharia2012resilient}. 