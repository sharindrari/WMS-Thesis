%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Background and Related Work} 

\section{Workflow Design}
\cite{yu2005taxonomy} classifies the workflow design to at least three taxonomies, namely: (a) workflow structure, (b) workflow model/specification, and (c) workflow composition system. In the view of workflow structure, a workflow is composed by connecting multiple tasks according to their dependencies. In general, the workflow can be represented as a DAG or non-DAG. The workflow that we develop in this thesis belongs to non-DAG-based workflow. In a non-DAG workflow, workflow structure is categorized into sequence, parallelism, choice, and iteration. Sequence is defined as an ordered series of tasks, with one task starting after a previous task has completed. Parallelism represents tasks which are performed concurrently, rather than serially. In choice control pattern, a task is selected to execute at run-time when its associated conditions are true. In Iteration structure, also known as loop, sections of workflow tasks in an iteration block are allowed to be repeated and is often occurred in workflow of complex use cases \cite{yu2005taxonomy}.  

Workflow Model, which is also called workflow specification, defines a workflow including its task definition and structure definition. There are two types of workflow models, namely abstract model and concrete model, denoted as abstract workflow and concrete workflow, respectively \cite{deelman2004workflow}. In the abstract model, a workflow is described in an abstract form, in which the workflow is specified without referring to specific resources for task execution. The abstract model enables users to define workflows without being concerned about low-level implementation details. In contrast, the concrete model binds workflow tasks to specific resources \cite{yu2005taxonomy}. In this thesis, we implement the abstract workflow which takes the form of control flow enriched dataflow. Furthermore, we also provide the algorithm to achieve the concrete workflow, namely the job scripts to be executed in the underlying system. 

Workflow composition systems are designed for enabling users to assemble components into workflows \cite{yu2005taxonomy}. It consists of two classes: (1) User-directed and (2) automatic.  User-directed composition systems allow users to edit workflows directly, whereas automatic composition systems generate workflows for users automatically \cite{yu2005taxonomy}. Both classes are implemented in this thesis. More specifically, in the user-directed, language-based modeling is applied since we require user to write their program in our Scala DSL. However, the control flow and data dependencies will later be identified automatically by performing the static analysis of the Scala program. In this case, user does not have to specify manually the workflow components and dependencies. To summarize, the taxonomy of workflow that we develop in this thesis is depicted in Figure \ref{fig:Taxonomy}.

\begin{figure}[here]
\centering
\includegraphics[width=1.0\linewidth]{figures/taxonomy}
\caption{Thesis Workflow Design \cite{yu2005taxonomy}}
\label{fig:Taxonomy}
\end{figure}

\section{Control Flow vs Data Flow}
In addition to the three taxonomies described in the previous section, most, if not all, workflow design belongs to one of two classes: control flow or data flow. The two classes are similar in that they specify the interaction between individual tasks within the group that comprise the workflow. The difference between the two is in their methods of implementing that interaction. In control-driven workflows, or control flows, the connections between the activities in a workflow represent a transfer of control from the preceding task to successor task. This includes control structures such as sequences, conditionals, and iterations. Data-driven workflows, or data flows, on the other hand, are designed to support data-driven applications. The dependencies represent the flow of data between workflow activities from data producer to data consumer \cite{deelman2009workflows}. More details explanation on control flow and data flows are presented in the following sections. 

\subsection{Control Flow}\label{ch1:opts}
Most control flow languages provide support not only for simple flows of control between components or services in the workflow but also for more complex control interactions such as iterations and conditionals. Users of workflow systems often require more than the simple control constructs that are available to them. The ability to perform branching in the workflow based on conditions and loop over sections of the workflow repeatedly is important for all applications especially for complex use cases \cite{deelman2009workflows}. The issue is not whether these facilities should exist but how to represent them in the workflow language and to what degree the language should support them. For instance, there is a question to whether a single simple loop construct is  enough, or whether the language should support all loop types(i.e. while, do while, for). In the case of conditional behaviour, the problem is determining whether the incoming value and the conditional value are equivalent \cite{deelman2009workflows}. 

\subsubsection{Iterations}

\subsubsection{Conditionals}

\subsection{Data Flow Model}
Most data flow representations are very simple in nature, and unlike their control flow counterparts, contain nothing apart from component or service descriptions and the data dependencies between them; control constructs such as loops are generally not included. In Triana’s workflow language, there are no control constructs at all; the dependencies between tasks are data dependencies, ensuring the data producer has finished before the consumer may start. Looping and conditional behaviour is performed through the use of specific components; a branch component with two or more output connections will output data on different connections, depending upon some condition. Loops are handled by making a circular connection in the workflow and having a conditional component break the loop upon a finishing condition, outputting to continue normal workflow execution. The benefit of both of these solutions to control behaviour in data flows is that the language representations remain simple. The downside is that the potential for running the workflow on different systems is reduced since the other system must have access not only to the workflow but to the components or services that perform the control operations \cite{deelman2009workflows}.

A dataflow is a directed acyclic graph (DAG) that consists of operators, sources, and sinks. The data sources and sinks, as well as the intermediate data sets that flow through operators, are bags of records. The operators are the inner nodes in the DAG, and can be thought of as functions f : {I1,...,In} → O, where Ii and O are bags of records. A dataflow construct of several operators is therefore a function, whose fixpoint we can find by “closing the loop” in the dataflow DAG \cite{ewen2012spinning}.

\section{Related Work}
There are some major existing dataflow systems and workflow systems that are interesting to mention. This section presents these different dataflow and workflow systems and discuss their characteristics and relate them to the DSL that we intend to build in this thesis. 

\subsection{Dataflow Systems}
Pig is a procedural data flow system for MapReduce \cite{gates2009building}. It offers a SQL-stle high-level data manipulation constructs, which can be assembled in any 

Pig is a high-level dataflow system that aims at a sweet spot between SQL and Map-Reduce. Pig offers SQL-style high-level data manipulation constructs, which can be as- sembled in an explicit dataflow and interleaved with custom Map- and Reduce-style functions or executables. Pig pro- grams are compiled into sequences of Map-Reduce jobs, and executed in the Hadoop Map-Reduce environment.

\subsection{Workflow Systems}
In recent years, a number of WMSs have emerged in the Big Data community and they are developed to run on top of Hadoop and/or for more general purpose. Within the Hadoop community, a WMS called Apache Oozie is developed to enable user to combine multiple MapReduce jobs into a logical unit of work to accomplish larger tasks or a workflow \cite{islam2012oozie}. Oozie is a Java Web Application that stores the workflow definitions and the currently running workflow instances, includ-ing their status (e.g. running, stalled, failed) and variables (e.g. input files, output files). An Oozie workflow is a sequence of actions (e.g. Hadoop MapReduce jobs, Pig jobs) represented in a control dependency DAG that is specified in the XML Process Definition Language. An Oozie workflow consists of Control Nodes and Action Nodes. Control nodes define the flow of execution and include start and end node of a workflow as well as the mechanisms to control the workflow execution path e.g. decision, fork, and join nodes whereas action Nodes are the mechanism to allow a workflow trigger the execution of a processing task \cite{islam2012oozie}. The Oozie WMS comprises three main components. The first component is a server that is responsible for launching jobs and detecting when datasets arrive in HDFS. The second component is a client, responsible for uploading data to HDFS; in Oozie, users upload their workflow definition to HDFS and use a REST API to submit the workflow to the Oozie server. Finally, the third component is a map task launcher for every single workflow that is run \cref{crobak}. 

- oozie state-oriented workflow with fork and join

Another example of a WMS that is not built specifically for Hadoop is Luigi and is developed by Spotify.  Luigi is a Python package that helps developers build complex pipelines of batch jobs \footnote{\label{luigi}https://github.com/spotify/luigi}. It facilitates developers to combine many tasks together, where each task may be a Hadoop job, a Hive query, loading a table from a database, etc. Luigi takes care of large portion of the workflow management so that the developer can focus on the tasks themselves and their dependencies. One major difference between Luigi and Oozie is that instead of XML configuration, the DAG in Luigi is specified with Python code constructs. This makes it easy to build complex dependency graphs of tasks. However, the workflow can trigger scripts that are not written in Python e.g., Pig scripts \cref{luigi}. The Luigi WMS consists of a centralized scheduler and a number of workers. The workers communicate with the scheduler over a JSON REST API. Each worker has the code base. In Luigi, the developer defines a job as a Python class. Luigi workers communicate with HDFS and walk through the work-flow DAG and check, for each task, whether a task’s output exists in order to determine the next tasks to be run. After it walks through the DAG, it then runs the tasks e.g. MapReduce jobs \cref{crobak}. 

XBaya supports a set of control primitives including for-each, conditional, while and exception. These control constructs are overlays on the dataflow graph to simplify the expression of the workflow. For example, a for-each construct can be used to encode a fan- out of a data flow graph where the degree of fan-out is not known until runtime. An exception construct is required when the workflow designer needs to express a sub-workflow alternative path when a part of the flow may be subject to potential, known runtime failures. BPEL has control structures including branching and looping built into the language but only for pre- defined fairly simple data types. For simple cases where we are comparing integers or simple strings, checking the condition is straightforward and unambiguous. The problem appears when we have to compare complex, structured scientific data in scientific workflows. This type of data often needs domain-specific knowledge in order to perform comparisons. Consequently, the evaluation of the comparison must be accomplished by an external service or agent, or a separate component, as part of the workflow execution. \cite{deelman2009workflows}.